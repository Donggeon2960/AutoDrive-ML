# YOLOv12 ì™„ì „ ê°€ì´ë“œ ğŸ“š

## ëª©ì°¨
1. [YOLOv12 ì†Œê°œ](#yolov12-ì†Œê°œ)
2. [YOLOv11ê³¼ì˜ ì£¼ìš” ì°¨ì´ì ](#yolov11ê³¼ì˜-ì£¼ìš”-ì°¨ì´ì )
3. [í•µì‹¬ ê¸°ìˆ  íŠ¹ì§•](#í•µì‹¬-ê¸°ìˆ -íŠ¹ì§•)
4. [ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •](#ì„¤ì¹˜-ë°-í™˜ê²½-ì„¤ì •)
5. [ì½”ë”© íŒê³¼ ì£¼ìš” í•¨ìˆ˜/ë³€ìˆ˜](#ì½”ë”©-íŒê³¼-ì£¼ìš”-í•¨ìˆ˜ë³€ìˆ˜)
6. [ì½”ë“œ ì˜ˆì‹œ](#ì½”ë“œ-ì˜ˆì‹œ)
7. [ì„±ëŠ¥ ë¹„êµ](#ì„±ëŠ¥-ë¹„êµ)
8. [ì‹¤ì œ í™œìš© ì‚¬ë¡€](#ì‹¤ì œ-í™œìš©-ì‚¬ë¡€)
9. [ë¬¸ì œ í•´ê²° ë° íŒ](#ë¬¸ì œ-í•´ê²°-ë°-íŒ)

---

## YOLOv12 ì†Œê°œ

YOLOv12ëŠ” 2025ë…„ 2ì›”ì— ë°œí‘œëœ ìµœì‹  ê°ì²´ íƒì§€ ëª¨ë¸ë¡œ, **ì–´í…ì…˜ ì¤‘ì‹¬(Attention-Centric)** ì•„í‚¤í…ì²˜ë¥¼ ë„ì…í•œ íšê¸°ì ì¸ ëª¨ë¸ì…ë‹ˆë‹¤. ê¸°ì¡´ YOLO ì‹œë¦¬ì¦ˆê°€ ì£¼ë¡œ CNN ê¸°ë°˜ì´ì—ˆë‹¤ë©´, YOLOv12ëŠ” Transformerì˜ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ íš¨ìœ¨ì ìœ¼ë¡œ í†µí•©í•˜ì—¬ ë” ì •í™•í•˜ê³  ë¹ ë¥¸ ê°ì²´ íƒì§€ë¥¼ ì‹¤í˜„í–ˆìŠµë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§• ìš”ì•½
- **Area Attention (AÂ²) ë©”ì»¤ë‹ˆì¦˜**: ê³„ì‚° ë¹„ìš©ì„ ì¤„ì´ë©´ì„œ í° ìˆ˜ìš© ì˜ì—­ ìœ ì§€
- **R-ELAN ì•„í‚¤í…ì²˜**: í–¥ìƒëœ íŠ¹ì§• ì§‘ê³„ì™€ ì•ˆì •ì ì¸ í›ˆë ¨
- **FlashAttention ì§€ì›**: ë©”ëª¨ë¦¬ ì ‘ê·¼ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
- **5ê°€ì§€ ëª¨ë¸ ë³€í˜•**: YOLOv12n, s, m, l, x (nanoë¶€í„° extra-largeê¹Œì§€)

---

## YOLOv11ê³¼ì˜ ì£¼ìš” ì°¨ì´ì 

### 1. ì•„í‚¤í…ì²˜ ì ‘ê·¼ë²•
| íŠ¹ì§• | YOLOv11 | YOLOv12 |
|------|---------|---------|
| **ê¸°ë³¸ êµ¬ì¡°** | Transformer ê¸°ë°˜ ë°±ë³¸ | ì–´í…ì…˜ ì¤‘ì‹¬ ì•„í‚¤í…ì²˜ |
| **ì–´í…ì…˜ ë°©ì‹** | ì„ íƒì  ì–´í…ì…˜ ì ìš© | Area Attention (AÂ²) ì „ì—­ ì ìš© |
| **íŠ¹ì§• ì§‘ê³„** | C3k2 ë¸”ë¡, C2PSA ëª¨ë“ˆ | R-ELAN (Residual ELAN) |
| **ë©”ëª¨ë¦¬ ìµœì í™”** | í‘œì¤€ ì–´í…ì…˜ | FlashAttention ì§€ì› |

### 2. ì„±ëŠ¥ ì°¨ì´ì 
YOLOv12nì€ YOLOv11n ëŒ€ë¹„ +1.2% mAP í–¥ìƒì„ ë³´ì—¬ì£¼ë©°, YOLOv10n ëŒ€ë¹„ë¡œëŠ” +2.1% ì„±ëŠ¥ ê°œì„ ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

**ì†ë„ vs ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„**:
- **YOLOv11**: ë” ë†’ì€ ì •í™•ë„ì— ì´ˆì 
- **YOLOv12**: ì†ë„ì™€ íš¨ìœ¨ì„± ìµœì í™”ì— ì¤‘ì 

### 3. í›ˆë ¨ ë° ì¶”ë¡  ì°¨ì´ì 
- **YOLOv11**: NMS ì œê±°ë¡œ ì¶”ë¡  ì†ë„ í–¥ìƒ
- **YOLOv12**: FlashAttentionìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œì™€ ì²˜ë¦¬ ì†ë„ í–¥ìƒ

---

## í•µì‹¬ ê¸°ìˆ  íŠ¹ì§•

### 1. Area Attention (AÂ²) ë©”ì»¤ë‹ˆì¦˜

Area Attentionì€ íŠ¹ì§• ë§µì„ ì—¬ëŸ¬ ì˜ì—­ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í° ìˆ˜ìš© ì˜ì—­ì„ ìœ ì§€í•˜ë©´ì„œë„ ê³„ì‚° ë³µì¡ë„ë¥¼ í¬ê²Œ ì¤„ì´ëŠ” í˜ì‹ ì ì¸ ì–´í…ì…˜ ë°©ì‹ì…ë‹ˆë‹¤.

#### ì‘ë™ ì›ë¦¬:
```python
# ì˜ì‚¬ ì½”ë“œë¡œ Area Attention ê°œë… ì„¤ëª…
def area_attention(feature_map, num_regions=4):
    H, W = feature_map.shape[-2:]
    
    # íŠ¹ì§• ë§µì„ Lê°œ ì˜ì—­ìœ¼ë¡œ ë¶„í•  (ê¸°ë³¸ê°’: 4)
    if direction == 'horizontal':
        regions = feature_map.split(H // num_regions, dim=-2)
    else:  # vertical
        regions = feature_map.split(W // num_regions, dim=-1)
    
    # ê° ì˜ì—­ì— ë…ë¦½ì ìœ¼ë¡œ ì–´í…ì…˜ ì ìš©
    attended_regions = []
    for region in regions:
        attended_region = self_attention(region)
        attended_regions.append(attended_region)
    
    return concat(attended_regions)
```

**ì¥ì **:
- ê³„ì‚° ë³µì¡ë„: `O(nÂ²hd/2)` (ê¸°ì¡´ `O(2nÂ²hd)` ëŒ€ë¹„ 4ë°° ê°ì†Œ)
- í° ìˆ˜ìš© ì˜ì—­ ìœ ì§€
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ

### 2. R-ELAN (Residual Efficient Layer Aggregation Networks)

R-ELANì€ ê¸°ì¡´ ELANì„ ê°œì„ í•˜ì—¬ íŠ¹íˆ ëŒ€ê·œëª¨ ì–´í…ì…˜ ì¤‘ì‹¬ ëª¨ë¸ì—ì„œ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

#### ì£¼ìš” ê°œì„ ì‚¬í•­:
- **ë¸”ë¡ ë ˆë²¨ ì”ì°¨ ì—°ê²°**: ë ˆì´ì–´ ìŠ¤ì¼€ì¼ë§ê³¼ ìœ ì‚¬í•œ ê¸°ë²•
- **ì¬ì„¤ê³„ëœ íŠ¹ì§• ì§‘ê³„**: ë³‘ëª© êµ¬ì¡° ìƒì„±
- **ì•ˆì •ì ì¸ í›ˆë ¨**: ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ìµœì í™”

### 3. FlashAttention í†µí•©

FlashAttentionì€ ë©”ëª¨ë¦¬ ì ‘ê·¼ ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì—¬ ì–´í…ì…˜ì˜ ì£¼ìš” ë©”ëª¨ë¦¬ ë³‘ëª©ì„ í•´ê²°í•©ë‹ˆë‹¤.

**ì§€ì› GPU**:
- Turing (T4, Quadro RTX ì‹œë¦¬ì¦ˆ)
- Ampere (RTX30 ì‹œë¦¬ì¦ˆ, A30/40/100)
- Ada Lovelace (RTX40 ì‹œë¦¬ì¦ˆ)
- Hopper (H100/H200)

---

## ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### 1. ê¸°ë³¸ ì„¤ì¹˜

```bash
# Ultralytics ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install ultralytics

# YOLOv12 íŠ¹í™” ì„¤ì¹˜ (FlashAttention í¬í•¨)
pip install ultralytics[flash-attention]

# ë˜ëŠ” GitHubì—ì„œ ì§ì ‘ ì„¤ì¹˜
git clone https://github.com/ultralytics/ultralytics.git
cd ultralytics
pip install -e .
```

### 2. GPU í™˜ê²½ í™•ì¸

```python
import torch
import ultralytics

# CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")

# FlashAttention ì§€ì› ì—¬ë¶€ í™•ì¸ (YOLOv12 ì „ìš©)
def check_flash_attention_support():
    if not torch.cuda.is_available():
        return False
    
    gpu_name = torch.cuda.get_device_name(0)
    supported_gpus = ['T4', 'RTX', 'A30', 'A40', 'A100', 'H100', 'H200']
    
    return any(gpu in gpu_name for gpu in supported_gpus)

print(f"FlashAttention supported: {check_flash_attention_support()}")
```

---

## ì½”ë”© íŒê³¼ ì£¼ìš” í•¨ìˆ˜/ë³€ìˆ˜

### 1. ëª¨ë¸ ì´ˆê¸°í™” ë° ì£¼ìš” íŒŒë¼ë¯¸í„°

```python
from ultralytics import YOLO

# YOLOv12 ëª¨ë¸ ì´ˆê¸°í™”
class YOLOv12Manager:
    def __init__(self, model_size='n', task='detect'):
        """
        YOLOv12 ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            model_size (str): ëª¨ë¸ í¬ê¸° ('n', 's', 'm', 'l', 'x')
            task (str): íƒœìŠ¤í¬ íƒ€ì… ('detect', 'segment', 'classify', 'pose', 'obb')
        """
        self.model_size = model_size
        self.task = task
        self.model_name = f"yolov12{model_size}.pt"
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = YOLO(self.model_name)
        
        # ëª¨ë¸ ì„¤ì •
        self.setup_model()
    
    def setup_model(self):
        """ëª¨ë¸ ì„¤ì • ìµœì í™”"""
        # FlashAttention í™œì„±í™” (ì§€ì›ë˜ëŠ” GPUì—ì„œ)
        if hasattr(self.model.model, 'use_flash_attention'):
            self.model.model.use_flash_attention = True
        
        # ì¶”ë¡  ìµœì í™” ì„¤ì •
        self.model.model.eval()
        if torch.cuda.is_available():
            self.model.model.cuda()
```

### 2. í•µì‹¬ í•¨ìˆ˜ì™€ ë³€ìˆ˜ ì„¤ëª…

#### ì£¼ìš” í´ë˜ìŠ¤ ë³€ìˆ˜
```python
# ëª¨ë¸ êµ¬ì„± ê´€ë ¨ ì£¼ìš” ë³€ìˆ˜
MODEL_CONFIGS = {
    'yolov12n': {
        'parameters': '3.0M',      # íŒŒë¼ë¯¸í„° ìˆ˜
        'mAP': 40.6,               # COCO mAP50-95
        'speed_t4': 1.64,          # T4 GPU ì¶”ë¡  ì‹œê°„ (ms)
        'use_case': 'edge_devices'  # ê¶Œì¥ ì‚¬ìš© í™˜ê²½
    },
    'yolov12s': {
        'parameters': '11.1M',
        'mAP': 46.8,
        'speed_t4': 2.1,
        'use_case': 'mobile_apps'
    },
    'yolov12m': {
        'parameters': '27.5M',
        'mAP': 51.0,
        'speed_t4': 3.8,
        'use_case': 'general_purpose'
    },
    'yolov12l': {
        'parameters': '46.2M',
        'mAP': 53.4,
        'speed_t4': 6.2,
        'use_case': 'high_accuracy'
    },
    'yolov12x': {
        'parameters': '72.3M',
        'mAP': 54.7,
        'speed_t4': 10.1,
        'use_case': 'maximum_accuracy'
    }
}
```

#### ì¤‘ìš” í•¨ìˆ˜ë“¤
```python
def optimize_for_inference(model, img_size=640):
    """
    ì¶”ë¡ ì„ ìœ„í•œ ëª¨ë¸ ìµœì í™”
    
    Args:
        model: YOLO ëª¨ë¸ ê°ì²´
        img_size (int): ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°
    
    Returns:
        ìµœì í™”ëœ ëª¨ë¸
    """
    # ë°˜ì •ë°€ë„ ì¶”ë¡  (GPU ë©”ëª¨ë¦¬ ì ˆì•½)
    if torch.cuda.is_available():
        model.half()
    
    # ì›Œë°ì—… (ì²« ì¶”ë¡  ì‹œê°„ ë‹¨ì¶•)
    dummy_img = torch.randn(1, 3, img_size, img_size)
    if torch.cuda.is_available():
        dummy_img = dummy_img.cuda().half()
    
    with torch.no_grad():
        _ = model(dummy_img)
    
    return model

def calculate_model_efficiency(results, inference_time):
    """
    ëª¨ë¸ íš¨ìœ¨ì„± ê³„ì‚°
    
    Args:
        results: ì¶”ë¡  ê²°ê³¼
        inference_time: ì¶”ë¡  ì‹œê°„ (ì´ˆ)
    
    Returns:
        íš¨ìœ¨ì„± ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
    """
    num_detections = len(results[0].boxes) if results[0].boxes is not None else 0
    fps = 1.0 / inference_time if inference_time > 0 else 0
    
    return {
        'fps': fps,
        'detections_per_second': num_detections * fps,
        'avg_confidence': float(results[0].boxes.conf.mean()) if num_detections > 0 else 0.0,
        'inference_time_ms': inference_time * 1000
    }
```

### 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìœ í‹¸ë¦¬í‹°

```python
import time
from collections import deque
import numpy as np

class PerformanceMonitor:
    """YOLOv12 ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, window_size=100):
        self.window_size = window_size
        self.inference_times = deque(maxlen=window_size)
        self.fps_history = deque(maxlen=window_size)
        self.memory_usage = deque(maxlen=window_size)
    
    def start_timing(self):
        """íƒ€ì´ë° ì‹œì‘"""
        self.start_time = time.perf_counter()
    
    def end_timing(self):
        """íƒ€ì´ë° ì¢…ë£Œ ë° ê¸°ë¡"""
        end_time = time.perf_counter()
        inference_time = end_time - self.start_time
        self.inference_times.append(inference_time)
        
        fps = 1.0 / inference_time if inference_time > 0 else 0
        self.fps_history.append(fps)
        
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1024**2  # MB
            self.memory_usage.append(memory_used)
        
        return inference_time
    
    def get_stats(self):
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        if not self.inference_times:
            return {}
        
        stats = {
            'avg_inference_time': np.mean(self.inference_times),
            'avg_fps': np.mean(self.fps_history),
            'min_fps': np.min(self.fps_history),
            'max_fps': np.max(self.fps_history),
            'std_fps': np.std(self.fps_history)
        }
        
        if self.memory_usage:
            stats.update({
                'avg_memory_mb': np.mean(self.memory_usage),
                'max_memory_mb': np.max(self.memory_usage)
            })
        
        return stats
```

---

## ì½”ë“œ ì˜ˆì‹œ

### 1. ê¸°ë³¸ ê°ì²´ íƒì§€

```python
from ultralytics import YOLO
import cv2
import numpy as np

def basic_detection_example():
    """ê¸°ë³¸ì ì¸ YOLOv12 ê°ì²´ íƒì§€ ì˜ˆì‹œ"""
    
    # ëª¨ë¸ ë¡œë“œ
    model = YOLO('yolov12n.pt')  # nano ë²„ì „ìœ¼ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
    
    # ì´ë¯¸ì§€ ë˜ëŠ” ë¹„ë””ì˜¤ ê²½ë¡œ
    source = 'path/to/your/image.jpg'  # ë˜ëŠ” 0 (ì›¹ìº ), 'video.mp4'
    
    # ì¶”ë¡  ì‹¤í–‰
    results = model(source)
    
    # ê²°ê³¼ ì²˜ë¦¬
    for result in results:
        # ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´
        boxes = result.boxes
        if boxes is not None:
            print(f"íƒì§€ëœ ê°ì²´ ìˆ˜: {len(boxes)}")
            
            for box in boxes:
                # í´ë˜ìŠ¤ ì •ë³´
                class_id = int(box.cls[0])
                class_name = model.names[class_id]
                confidence = float(box.conf[0])
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ (xyxy í˜•ì‹)
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                
                print(f"í´ë˜ìŠ¤: {class_name}, ì‹ ë¢°ë„: {confidence:.3f}, "
                      f"ì¢Œí‘œ: ({x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f})")
        
        # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
        result.save('output_image.jpg')

if __name__ == "__main__":
    basic_detection_example()
```

### 2. ì‹¤ì‹œê°„ ì›¹ìº  íƒì§€

```python
import cv2
from ultralytics import YOLO
import time

class RealTimeDetector:
    def __init__(self, model_size='n', conf_threshold=0.5):
        """
        ì‹¤ì‹œê°„ íƒì§€ê¸° ì´ˆê¸°í™”
        
        Args:
            model_size (str): ëª¨ë¸ í¬ê¸° ('n', 's', 'm', 'l', 'x')
            conf_threshold (float): ì‹ ë¢°ë„ ì„ê³„ê°’
        """
        self.model = YOLO(f'yolov12{model_size}.pt')
        self.conf_threshold = conf_threshold
        self.performance_monitor = PerformanceMonitor()
        
    def run(self, source=0):
        """
        ì‹¤ì‹œê°„ íƒì§€ ì‹¤í–‰
        
        Args:
            source: ë¹„ë””ì˜¤ ì†ŒìŠ¤ (0: ì›¹ìº , ë˜ëŠ” ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ)
        """
        cap = cv2.VideoCapture(source)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # ì¶”ë¡  ì‹œì‘
                self.performance_monitor.start_timing()
                
                # YOLOv12 ì¶”ë¡ 
                results = self.model(frame, conf=self.conf_threshold, verbose=False)
                
                # ì¶”ë¡  ì‹œê°„ ê¸°ë¡
                inference_time = self.performance_monitor.end_timing()
                
                # ê²°ê³¼ ì‹œê°í™”
                annotated_frame = self.visualize_results(frame, results[0])
                
                # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
                fps = 1.0 / inference_time if inference_time > 0 else 0
                cv2.putText(annotated_frame, f'FPS: {fps:.1f}', (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                
                # í™”ë©´ ì¶œë ¥
                cv2.imshow('YOLOv12 Real-time Detection', annotated_frame)
                
                # 'q' í‚¤ë¡œ ì¢…ë£Œ
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
                    
        finally:
            cap.release()
            cv2.destroyAllWindows()
            
            # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
            stats = self.performance_monitor.get_stats()
            print("\n=== ì„±ëŠ¥ í†µê³„ ===")
            for key, value in stats.items():
                print(f"{key}: {value:.3f}")
    
    def visualize_results(self, frame, result):
        """ê²°ê³¼ ì‹œê°í™”"""
        if result.boxes is not None:
            boxes = result.boxes
            
            for box in boxes:
                # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
                
                # í´ë˜ìŠ¤ ì •ë³´
                class_id = int(box.cls[0])
                class_name = self.model.names[class_id]
                confidence = float(box.conf[0])
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸
                label = f'{class_name}: {confidence:.2f}'
                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                
                # ë¼ë²¨ ë°°ê²½
                cv2.rectangle(frame, (x1, y1 - label_size[1] - 10), 
                             (x1 + label_size[0], y1), (0, 255, 0), -1)
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸
                cv2.putText(frame, label, (x1, y1 - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
        
        return frame

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    detector = RealTimeDetector(model_size='s', conf_threshold=0.3)
    detector.run(source=0)  # ì›¹ìº  ì‚¬ìš©
```

### 3. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í›ˆë ¨

```python
from ultralytics import YOLO
import yaml

def train_custom_yolov12():
    """ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ìœ¼ë¡œ YOLOv12 í›ˆë ¨"""
    
    # ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼ ìƒì„±
    dataset_config = {
        'train': 'path/to/train/images',
        'val': 'path/to/val/images',
        'test': 'path/to/test/images',  # ì„ íƒì‚¬í•­
        'nc': 2,  # í´ë˜ìŠ¤ ìˆ˜
        'names': ['person', 'bicycle']  # í´ë˜ìŠ¤ ì´ë¦„
    }
    
    # YAML íŒŒì¼ë¡œ ì €ì¥
    with open('custom_dataset.yaml', 'w') as f:
        yaml.dump(dataset_config, f)
    
    # ëª¨ë¸ ì´ˆê¸°í™” (ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì—ì„œ ì‹œì‘)
    model = YOLO('yolov12n.pt')
    
    # í›ˆë ¨ íŒŒë¼ë¯¸í„° ì„¤ì •
    train_params = {
        'data': 'custom_dataset.yaml',
        'epochs': 100,
        'batch': 16,
        'imgsz': 640,
        'device': 0,  # GPU 0 ì‚¬ìš©
        'workers': 8,
        'project': 'yolov12_custom',
        'name': 'experiment_1',
        'exist_ok': True,
        'pretrained': True,
        'optimizer': 'AdamW',
        'lr0': 0.001,
        'weight_decay': 0.0005,
        'warmup_epochs': 3,
        'warmup_momentum': 0.8,
        'box': 7.5,      # box loss gain
        'cls': 0.5,      # cls loss gain  
        'dfl': 1.5,      # dfl loss gain
        'hsv_h': 0.015,  # image HSV-Hue augmentation
        'hsv_s': 0.7,    # image HSV-Saturation augmentation
        'hsv_v': 0.4,    # image HSV-Value augmentation
        'degrees': 0.0,  # image rotation (+/- deg)
        'translate': 0.1, # image translation (+/- fraction)
        'scale': 0.5,    # image scale (+/- gain)
        'shear': 0.0,    # image shear (+/- deg)
        'perspective': 0.0, # image perspective (+/- fraction)
        'flipud': 0.0,   # image flip up-down (probability)
        'fliplr': 0.5,   # image flip left-right (probability)
        'mosaic': 1.0,   # image mosaic (probability)
        'mixup': 0.0,    # image mixup (probability)
        'copy_paste': 0.0 # segment copy-paste (probability)
    }
    
    # í›ˆë ¨ ì‹¤í–‰
    try:
        results = model.train(**train_params)
        print("í›ˆë ¨ ì™„ë£Œ!")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
        best_model = YOLO('yolov12_custom/experiment_1/weights/best.pt')
        
        # ê²€ì¦ ìˆ˜í–‰
        val_results = best_model.val()
        print(f"ê²€ì¦ mAP50: {val_results.box.map50:.3f}")
        print(f"ê²€ì¦ mAP50-95: {val_results.box.map:.3f}")
        
        return best_model
        
    except Exception as e:
        print(f"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ
def use_trained_model():
    """í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©"""
    model = YOLO('yolov12_custom/experiment_1/weights/best.pt')
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ì¶”ë¡ 
    results = model('path/to/test/image.jpg')
    
    # ê²°ê³¼ ì €ì¥
    results[0].save('prediction.jpg')

if __name__ == "__main__":
    trained_model = train_custom_yolov12()
    if trained_model:
        use_trained_model()
```

### 4. ë°°ì¹˜ ì²˜ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”

```python
import torch
from pathlib import Path
import time

class BatchProcessor:
    """YOLOv12 ë°°ì¹˜ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path='yolov12n.pt', batch_size=8, device='auto'):
        self.model = YOLO(model_path)
        self.batch_size = batch_size
        self.device = device
        
        # ëª¨ë¸ ìµœì í™”
        self.optimize_model()
    
    def optimize_model(self):
        """ëª¨ë¸ ìµœì í™” ì„¤ì •"""
        if torch.cuda.is_available() and self.device != 'cpu':
            # GPU ì‚¬ìš© ì‹œ ë°˜ì •ë°€ë„ ì„¤ì •
            self.model.model.half()
            torch.backends.cudnn.benchmark = True
    
    def process_image_folder(self, folder_path, output_folder=None, conf_threshold=0.5):
        """
        í´ë” ë‚´ ëª¨ë“  ì´ë¯¸ì§€ ë°°ì¹˜ ì²˜ë¦¬
        
        Args:
            folder_path (str): ì…ë ¥ ì´ë¯¸ì§€ í´ë” ê²½ë¡œ
            output_folder (str): ì¶œë ¥ í´ë” ê²½ë¡œ
            conf_threshold (float): ì‹ ë¢°ë„ ì„ê³„ê°’
        """
        folder_path = Path(folder_path)
        if output_folder:
            output_path = Path(output_folder)
            output_path.mkdir(exist_ok=True)
        
        # ì§€ì›í•˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ì
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        image_files = [f for f in folder_path.glob('*') if f.suffix.lower() in image_extensions]
        
        print(f"ì²˜ë¦¬í•  ì´ë¯¸ì§€ ìˆ˜: {len(image_files)}")
        
        # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
        total_time = 0
        total_images = 0
        
        for i in range(0, len(image_files), self.batch_size):
            batch_files = image_files[i:i+self.batch_size]
            
            start_time = time.time()
            
            # ë°°ì¹˜ ì¶”ë¡ 
            batch_paths = [str(f) for f in batch_files]
            results = self.model(batch_paths, conf=conf_threshold, verbose=False)
            
            batch_time = time.time() - start_time
            total_time += batch_time
            total_images += len(batch_files)
            
            # ê²°ê³¼ ì €ì¥
            if output_folder:
                for j, result in enumerate(results):
                    output_file = output_path / f"result_{batch_files[j].stem}.jpg"
                    result.save(str(output_file))
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            avg_fps = total_images / total_time if total_time > 0 else 0
            print(f"ë°°ì¹˜ {i//self.batch_size + 1}/{(len(image_files)-1)//self.batch_size + 1} "
                  f"ì™„ë£Œ, í‰ê·  FPS: {avg_fps:.2f}")
        
        print(f"\nì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"í‰ê·  FPS: {total_images/total_time:.2f}")
        
        return results

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    processor = BatchProcessor(
        model_path='yolov12s.pt',
        batch_size=4,
        device='auto'
    )
    
    processor.process_image_folder(
        folder_path='input_images/',
        output_folder='output_results/',
        conf_threshold=0.3
    )
```

---

## ì„±ëŠ¥ ë¹„êµ

### YOLOv11 vs YOLOv12 ë²¤ì¹˜ë§ˆí¬

COCO val2017 ë°ì´í„°ì…‹ì—ì„œì˜ ì„±ëŠ¥ ë¹„êµ:

| ëª¨ë¸ | mAP50-95 | íŒŒë¼ë¯¸í„° ìˆ˜ | ì¶”ë¡  ì‹œê°„ (T4) | ê°œì„ ì‚¬í•­ |
|------|----------|-------------|----------------|----------|
| YOLOv11n | 39.4% | 2.6M | 1.65ms | - |
| **YOLOv12n** | **40.6%** | **3.0M** | **1.64ms** | **+1.2% mAP** |
| YOLOv11s | 47.0% | 9.4M | 2.1ms | - |
| **YOLOv12s** | **46.8%** | **11.1M** | **2.1ms** | **ì†ë„ ìœ ì§€** |
| YOLOv11m | 51.5% | 20.1M | 4.2ms | - |
| **YOLOv12m** | **51.0%** | **27.5M** | **3.8ms** | **ë” ë¹ ë¥¸ ì¶”ë¡ ** |

### ë‹¤ë¥¸ ëª¨ë¸ê³¼ì˜ ë¹„êµ

YOLOv12ëŠ” RT-DETR ëŒ€ë¹„ 42% ë¹ ë¥¸ ì†ë„ë¥¼ ë³´ì´ë©°, 23.4% ì ì€ ê³„ì‚°ëŸ‰ê³¼ 22.2% ì ì€ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

---

## ì‹¤ì œ í™œìš© ì‚¬ë¡€

### 1. ììœ¨ì£¼í–‰ì°¨ëŸ‰
```python
class AutonomousVehicleDetector:
    """ììœ¨ì£¼í–‰ìš© YOLOv12 íƒì§€ê¸°"""
    
    def __init__(self):
        # ë†’ì€ ì •í™•ë„ë¥¼ ìœ„í•´ large ëª¨ë¸ ì‚¬ìš©
        self.model = YOLO('yolov12l.pt')
        self.vehicle_classes = ['car', 'truck', 'bus', 'motorcycle', 'bicycle']
        self.person_classes = ['person']
        self.traffic_classes = ['traffic light', 'stop sign']
    
    def detect_traffic_objects(self, frame):
        """êµí†µ ê°ì²´ íƒì§€"""
        results = self.model(frame, conf=0.4)
        
        detected_objects = {
            'vehicles': [],
            'pedestrians': [],
            'traffic_signs': []
        }
        
        if results[0].boxes is not None:
            for box in results[0].boxes:
                class_name = self.model.names[int(box.cls[0])]
                confidence = float(box.conf[0])
                bbox = box.xyxy[0].cpu().numpy()
                
                if class_name in self.vehicle_classes:
                    detected_objects['vehicles'].append({
                        'class': class_name,
                        'confidence': confidence,
                        'bbox': bbox
                    })
                elif class_name in self.person_classes:
                    detected_objects['pedestrians'].append({
                        'class': class_name,
                        'confidence': confidence,
                        'bbox': bbox
                    })
                elif class_name in self.traffic_classes:
                    detected_objects['traffic_signs'].append({
                        'class': class_name,
                        'confidence': confidence,
                        'bbox': bbox
                    })
        
        return detected_objects
```

### 2. ë³´ì•ˆ ê°ì‹œ ì‹œìŠ¤í…œ
```python
class SecurityMonitor:
    """ë³´ì•ˆ ê°ì‹œìš© YOLOv12 ì‹œìŠ¤í…œ"""
    
    def __init__(self, alert_threshold=0.7):
        # ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ small ëª¨ë¸ ì‚¬ìš©
        self.model = YOLO('yolov12s.pt')
        self.alert_threshold = alert_threshold
        self.suspicious_classes = ['person', 'backpack', 'suitcase']
        
    def monitor_area(self, frame, restricted_zone=None):
        """ì œí•œ êµ¬ì—­ ëª¨ë‹ˆí„°ë§"""
        results = self.model(frame, conf=0.3)
        alerts = []
        
        if results[0].boxes is not None:
            for box in results[0].boxes:
                class_name = self.model.names[int(box.cls[0])]
                confidence = float(box.conf[0])
                
                if (class_name in self.suspicious_classes and 
                    confidence > self.alert_threshold):
                    
                    # ì œí•œ êµ¬ì—­ ì¹¨ì… í™•ì¸
                    if restricted_zone:
                        bbox = box.xyxy[0].cpu().numpy()
                        if self.is_in_restricted_zone(bbox, restricted_zone):
                            alerts.append({
                                'type': 'restricted_zone_intrusion',
                                'class': class_name,
                                'confidence': confidence,
                                'timestamp': time.time()
                            })
        
        return alerts
    
    def is_in_restricted_zone(self, bbox, zone):
        """ì œí•œ êµ¬ì—­ ì¹¨ì… ì—¬ë¶€ í™•ì¸"""
        x1, y1, x2, y2 = bbox
        center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2
        
        return (zone['x1'] <= center_x <= zone['x2'] and 
                zone['y1'] <= center_y <= zone['y2'])
```

### 3. ì˜ë£Œ ì´ë¯¸ì§•
```python
class MedicalImageAnalyzer:
    """ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„ìš© YOLOv12"""
    
    def __init__(self, custom_model_path):
        # ì˜ë£Œ ì˜ìƒì— íŠ¹í™”ëœ ì»¤ìŠ¤í…€ ëª¨ë¸ ì‚¬ìš©
        self.model = YOLO(custom_model_path)
        
    def analyze_xray(self, xray_image, confidence_threshold=0.6):
        """X-ray ì´ë¯¸ì§€ ë¶„ì„"""
        results = self.model(xray_image, conf=confidence_threshold)
        
        findings = []
        if results[0].boxes is not None:
            for box in results[0].boxes:
                class_name = self.model.names[int(box.cls[0])]
                confidence = float(box.conf[0])
                bbox = box.xyxy[0].cpu().numpy()
                
                findings.append({
                    'finding': class_name,
                    'confidence': confidence,
                    'location': bbox,
                    'severity': self.assess_severity(class_name, confidence)
                })
        
        return findings
    
    def assess_severity(self, finding, confidence):
        """ì†Œê²¬ì˜ ì‹¬ê°ë„ í‰ê°€"""
        severity_map = {
            'pneumonia': 'high' if confidence > 0.8 else 'medium',
            'fracture': 'high' if confidence > 0.9 else 'medium',
            'nodule': 'medium' if confidence > 0.7 else 'low'
        }
        return severity_map.get(finding, 'low')
```

---

## ë¬¸ì œ í•´ê²° ë° íŒ

### 1. ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
def handle_gpu_memory_error():
    """GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ í•´ê²° ë°©ë²•"""
    
    # 1. ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
    batch_size = 1
    
    # 2. ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê¸°
    img_size = 320  # ê¸°ë³¸ê°’ 640ì—ì„œ ì¶•ì†Œ
    
    # 3. ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸°
    model = YOLO('yolov12n.pt')  # nano ëª¨ë¸ ì‚¬ìš©
    
    # 4. ë°˜ì •ë°€ë„ ì¶”ë¡ 
    if torch.cuda.is_available():
        model.model.half()
    
    # 5. ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    
    return model
```

#### FlashAttention ì„¤ì¹˜ ë¬¸ì œ
```bash
# CUDA ë²„ì „ í™•ì¸
nvidia-smi

# í˜¸í™˜ë˜ëŠ” FlashAttention ì„¤ì¹˜
pip install flash-attn --no-build-isolation

# ë˜ëŠ” conda ì‚¬ìš©
conda install flash-attn -c pytorch
```

### 2. ì„±ëŠ¥ ìµœì í™” íŒ

#### ëª¨ë¸ ë‚´ë³´ë‚´ê¸° ë° ë°°í¬
```python
def optimize_for_deployment():
    """ë°°í¬ë¥¼ ìœ„í•œ ëª¨ë¸ ìµœì í™”"""
    
    model = YOLO('yolov12s.pt')
    
    # TensorRTë¡œ ë‚´ë³´ë‚´ê¸° (NVIDIA GPU)
    model.export(format='engine', half=True, device=0)
    
    # ONNXë¡œ ë‚´ë³´ë‚´ê¸° (ë²”ìš©)
    model.export(format='onnx', dynamic=True, simplify=True)
    
    # CoreMLë¡œ ë‚´ë³´ë‚´ê¸° (Apple ê¸°ê¸°)
    model.export(format='coreml', nms=True)
    
    # ë‚´ë³´ë‚¸ ëª¨ë¸ ì‚¬ìš©
    tensorrt_model = YOLO('yolov12s.engine')
    onnx_model = YOLO('yolov12s.onnx')
    
    return tensorrt_model
```

#### ë©€í‹°í”„ë¡œì„¸ì‹± í™œìš©
```python
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor

def process_video_parallel(video_path, num_processes=4):
    """ë¹„ë””ì˜¤ ë³‘ë ¬ ì²˜ë¦¬"""
    
    def process_frame_batch(frame_batch):
        model = YOLO('yolov12n.pt')
        results = []
        for frame in frame_batch:
            result = model(frame, verbose=False)
            results.append(result)
        return results
    
    # ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì¶œ
    cap = cv2.VideoCapture(video_path)
    frames = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(frame)
    cap.release()
    
    # í”„ë ˆì„ì„ ë°°ì¹˜ë¡œ ë¶„í• 
    batch_size = len(frames) // num_processes
    frame_batches = [frames[i:i+batch_size] for i in range(0, len(frames), batch_size)]
    
    # ë³‘ë ¬ ì²˜ë¦¬
    with ProcessPoolExecutor(max_workers=num_processes) as executor:
        batch_results = list(executor.map(process_frame_batch, frame_batches))
    
    # ê²°ê³¼ ë³‘í•©
    all_results = []
    for batch_result in batch_results:
        all_results.extend(batch_result)
    
    return all_results
```

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```python
def hyperparameter_tuning_guide():
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ"""
    
    # ê¸°ë³¸ ì„¤ì •
    base_params = {
        'epochs': 100,
        'batch': 16,
        'lr0': 0.01,
        'weight_decay': 0.0005,
        'momentum': 0.937,
        'warmup_epochs': 3,
        'warmup_momentum': 0.8,
        'warmup_bias_lr': 0.1,
        'box': 7.5,
        'cls': 0.5,
        'dfl': 1.5
    }
    
    # ë°ì´í„°ì…‹ í¬ê¸°ë³„ ì¡°ì •
    small_dataset_adjustments = {
        'epochs': 200,  # ë” ë§ì€ ì—í­
        'lr0': 0.001,   # ë‚®ì€ í•™ìŠµë¥ 
        'batch': 8,     # ì‘ì€ ë°°ì¹˜
        'warmup_epochs': 5
    }
    
    # í° ë°ì´í„°ì…‹ ì¡°ì •
    large_dataset_adjustments = {
        'epochs': 50,   # ì ì€ ì—í­
        'lr0': 0.02,    # ë†’ì€ í•™ìŠµë¥ 
        'batch': 32,    # í° ë°°ì¹˜
        'warmup_epochs': 1
    }
    
    # ì‘ì€ ê°ì²´ íƒì§€ ìµœì í™”
    small_object_params = {
        'imgsz': 1024,  # í° ì´ë¯¸ì§€ í¬ê¸°
        'scale': 0.9,   # ì ì€ ìŠ¤ì¼€ì¼ ë³€í™”
        'mosaic': 0.5,  # ì¤„ì–´ë“  ëª¨ìì´í¬
        'mixup': 0.15   # ë¯¹ìŠ¤ì—… ì¶”ê°€
    }
    
    return {
        'base': base_params,
        'small_dataset': small_dataset_adjustments,
        'large_dataset': large_dataset_adjustments,
        'small_objects': small_object_params
    }
```

---

## ê²°ë¡ 

YOLOv12ëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ íš¨ìœ¨ì ìœ¼ë¡œ í†µí•©í•˜ì—¬ ê¸°ì¡´ YOLO ì‹œë¦¬ì¦ˆì˜ í•œê³„ë¥¼ ê·¹ë³µí•œ í˜ì‹ ì ì¸ ëª¨ë¸ì…ë‹ˆë‹¤. **Area Attention**, **R-ELAN**, **FlashAttention** ë“±ì˜ í•µì‹¬ ê¸°ìˆ ì„ í†µí•´ ë” ë¹ ë¥´ê³  ì •í™•í•œ ê°ì²´ íƒì§€ë¥¼ ì‹¤í˜„í–ˆìŠµë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì¥ì 
1. **ì†ë„ì™€ ì •í™•ë„ì˜ ê· í˜•**: YOLOv11 ëŒ€ë¹„ í–¥ìƒëœ mAPì™€ ìœ ì‚¬í•œ ì¶”ë¡  ì†ë„
2. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: FlashAttentionìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
3. **ë‹¤ì–‘í•œ í™œìš© ë¶„ì•¼**: ììœ¨ì£¼í–‰, ë³´ì•ˆ, ì˜ë£Œ ë“± ê´‘ë²”ìœ„í•œ ì‘ìš© ê°€ëŠ¥

### ì‚¬ìš© ê¶Œì¥ì‚¬í•­
- **YOLOv12n**: ì—£ì§€ ë””ë°”ì´ìŠ¤, ëª¨ë°”ì¼ ì• í”Œë¦¬ì¼€ì´ì…˜
- **YOLOv12s**: ì‹¤ì‹œê°„ ì²˜ë¦¬ê°€ ì¤‘ìš”í•œ ì¼ë°˜ì ì¸ ìš©ë„
- **YOLOv12m**: ì •í™•ë„ì™€ ì†ë„ì˜ ê· í˜•ì´ í•„ìš”í•œ ê²½ìš°
- **YOLOv12l/x**: ìµœê³  ì •í™•ë„ê°€ ìš”êµ¬ë˜ëŠ” ì „ë¬¸ ë¶„ì•¼

YOLOv12ë¥¼ í†µí•´ ë”ìš± íš¨ìœ¨ì ì´ê³  ì •í™•í•œ ì»´í“¨í„° ë¹„ì „ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ì‹œê¸° ë°”ëë‹ˆë‹¤! ğŸš€

---

## ì°¸ê³  ìë£Œ
- [YOLOv12 ê³µì‹ ë…¼ë¬¸](https://arxiv.org/abs/2502.12524)
- [Ultralytics YOLOv12 ë¬¸ì„œ](https://docs.ultralytics.com/models/yolo12/)
- [YOLOv12 GitHub ì €ì¥ì†Œ](https://github.com/sunsmarterjie/yolov12)
- [YOLOv11 vs YOLOv12 ë¹„êµ ë¶„ì„](https://www.analyticsvidhya.com/blog/2025/03/yolo-v12/)
